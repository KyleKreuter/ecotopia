[
  {
    "name": "ecotopia-extract-ministral-8b",
    "state": "failed",
    "config": {},
    "summary": {
      "_runtime": 0,
      "_wandb": "{'runtime': 0}"
    },
    "url": "https://wandb.ai/nolancacheux/hackathon-london-nolan-2026/runs/rnmb007t",
    "created_at": "2026-02-28T16:47:53Z",
    "job_type": null
  },
  {
    "name": "ecotopia-extract-ministral-8b",
    "state": "finished",
    "config": {
      "bf16": true,
      "fp16": false,
      "fsdp": [],
      "seed": 42,
      "tf32": null,
      "debug": [],
      "dtype": "bfloat16",
      "optim": "adamw_torch_fused",
      "do_eval": true,
      "packing": false,
      "project": "huggingface",
      "use_cpu": false,
      "do_train": false,
      "head_dim": 128,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "run_name": "ecotopia-extract-ministral-8b",
      "data_seed": null,
      "deepspeed": null,
      "eos_token": "<EOS_TOKEN>",
      "hub_token": "<HUB_TOKEN>",
      "log_level": "passive",
      "loss_type": "nll",
      "max_steps": -1,
      "pad_token": "<PAD_TOKEN>",
      "report_to": [
        "wandb"
      ],
      "use_cache": false,
      "adam_beta1": 0.9,
      "adam_beta2": 0.999,
      "do_predict": false,
      "eval_delay": 0,
      "eval_steps": 20,
      "hidden_act": "silu",
      "local_rank": -1,
      "max_length": 1024,
      "model_type": "ministral",
      "optim_args": null,
      "output_dir": "./ecotopia-extract-ministral-8b",
      "save_steps": 50,
      "vocab_size": 131072,
      "ddp_backend": null,
      "ddp_timeout": 1800,
      "fsdp_config": {
        "xla": false,
        "xla_fsdp_v2": false,
        "min_num_params": 0,
        "xla_fsdp_grad_ckpt": false
      },
      "hidden_size": 4096,
      "label_names": null,
      "layer_types": [
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention"
      ],
      "logging_dir": null,
      "peft_config": {
        "default": {
          "r": 16,
          "bias": "none",
          "revision": null,
          "use_dora": false,
          "lora_bias": false,
          "peft_type": "LORA",
          "task_type": "CAUSAL_LM",
          "eva_config": null,
          "lora_alpha": 32,
          "use_qalora": false,
          "use_rslora": false,
          "arrow_config": null,
          "auto_mapping": null,
          "corda_config": null,
          "lora_dropout": 0.05,
          "peft_version": "0.18.1",
          "megatron_core": "megatron.core",
          "fan_in_fan_out": false,
          "inference_mode": false,
          "layers_pattern": null,
          "runtime_config": {
            "ephemeral_gpu_offload": false
          },
          "target_modules": [
            "o_proj",
            "k_proj",
            "v_proj",
            "q_proj"
          ],
          "exclude_modules": null,
          "megatron_config": null,
          "modules_to_save": null,
          "init_lora_weights": true,
          "layer_replication": null,
          "qalora_group_size": 16,
          "target_parameters": null,
          "ensure_weight_tying": false,
          "layers_to_transform": null,
          "alora_invocation_tokens": null,
          "base_model_name_or_path": "mistralai/Ministral-8B-Instruct-2410",
          "trainable_token_indices": null
        }
      },
      "push_to_hub": true,
      "return_dict": true,
      "adam_epsilon": 1e-08,
      "bos_token_id": 1,
      "disable_tqdm": false,
      "eos_token_id": 2,
      "eval_packing": null,
      "hub_model_id": "mistral-hackaton-2026/ecotopia-extract-ministral-8b",
      "hub_revision": null,
      "hub_strategy": "end",
      "pad_token_id": 2,
      "padding_free": false,
      "problem_type": null,
      "rms_norm_eps": 1e-05,
      "warmup_ratio": 0.1,
      "warmup_steps": 0.1,
      "weight_decay": 0,
      "_name_or_path": "mistralai/Ministral-8B-Instruct-2410",
      "architectures": [
        "MistralForCausalLM"
      ],
      "eval_on_start": false,
      "eval_strategy": "steps",
      "learning_rate": 0.0002,
      "logging_steps": 5,
      "max_grad_norm": 1,
      "save_strategy": "steps",
      "torch_compile": false,
      "bf16_full_eval": false,
      "dataset_kwargs": null,
      "fp16_full_eval": false,
      "sliding_window": 32768,
      "hub_always_push": false,
      "rope_parameters": {
        "rope_type": "default",
        "rope_theta": 100000000
      },
      "save_only_model": false,
      "shuffle_dataset": false,
      "dataset_num_proc": null,
      "full_determinism": false,
      "hub_private_repo": null,
      "ignore_data_skip": false,
      "log_on_each_node": true,
      "logging_strategy": "steps",
      "num_train_epochs": 3,
      "packing_strategy": "bfd",
      "save_total_limit": null,
      "trackio_space_id": "trackio",
      "use_liger_kernel": false,
      "attention_dropout": 0,
      "ddp_bucket_cap_mb": null,
      "greater_is_better": null,
      "initializer_range": 0.02,
      "intermediate_size": 12288,
      "log_level_replica": "warning",
      "lr_scheduler_type": "cosine",
      "model_init_kwargs": null,
      "num_hidden_layers": 36,
      "output_attentions": false,
      "save_on_each_node": false,
      "accelerator_config": {
        "even_batches": true,
        "non_blocking": false,
        "split_batches": false,
        "dispatch_batches": null,
        "use_seedable_sampler": true,
        "gradient_accumulation_kwargs": null
      },
      "batch_eval_metrics": false,
      "chat_template_path": null,
      "dataset_text_field": "text",
      "is_encoder_decoder": false,
      "length_column_name": "length",
      "logging_first_step": false,
      "pad_to_multiple_of": null,
      "parallelism_config": null,
      "torch_compile_mode": null,
      "assistant_only_loss": false,
      "include_for_metrics": [],
      "liger_kernel_config": null,
      "lr_scheduler_kwargs": null,
      "neftune_noise_alpha": null,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "quantization_config": {
        "load_in_4bit": true,
        "load_in_8bit": false,
        "quant_method": "BITS_AND_BYTES",
        "_load_in_4bit": true,
        "_load_in_8bit": false,
        "llm_int8_threshold": 6,
        "bnb_4bit_quant_type": "nf4",
        "llm_int8_skip_modules": null,
        "bnb_4bit_compute_dtype": "bfloat16",
        "bnb_4bit_quant_storage": "uint8",
        "llm_int8_has_fp16_weight": false,
        "bnb_4bit_use_double_quant": true,
        "llm_int8_enable_fp32_cpu_offload": false
      },
      "skip_memory_metrics": true,
      "tie_word_embeddings": false,
      "auto_find_batch_size": false,
      "completion_only_loss": null,
      "dataloader_drop_last": false,
      "model/num_parameters": 8035143680,
      "optim_target_modules": null,
      "output_hidden_states": false,
      "prediction_loss_only": false,
      "transformers_version": "5.2.0",
      "activation_offloading": false,
      "dataloader_pin_memory": true,
      "ddp_broadcast_buffers": null,
      "enable_jit_checkpoint": false,
      "metric_for_best_model": null,
      "remove_unused_columns": true,
      "torch_compile_backend": null,
      "dataloader_num_workers": 0,
      "eval_do_concat_batches": true,
      "eval_use_gather_object": false,
      "gradient_checkpointing": true,
      "label_smoothing_factor": 0,
      "load_best_model_at_end": false,
      "logging_nan_inf_filter": true,
      "resume_from_checkpoint": null,
      "chunk_size_feed_forward": 0,
      "eval_accumulation_steps": null,
      "max_position_embeddings": 32768,
      "torch_empty_cache_steps": null,
      "train_sampling_strategy": "random",
      "dataloader_prefetch_factor": null,
      "ddp_find_unused_parameters": null,
      "per_device_eval_batch_size": 8,
      "gradient_accumulation_steps": 4,
      "per_device_train_batch_size": 2,
      "average_tokens_across_devices": true,
      "dataloader_persistent_workers": false,
      "gradient_checkpointing_kwargs": {
        "use_reentrant": false
      },
      "include_num_input_tokens_seen": "no",
      "restore_callback_states_from_checkpoint": false
    },
    "summary": {
      "_runtime": 208,
      "_step": 15,
      "_timestamp": 1772297716.0436556,
      "_wandb": "{'runtime': 208}",
      "eval/entropy": 0.5356223881244659,
      "eval/loss": 0.5526188015937805,
      "eval/mean_token_accuracy": 0.8560699939727783,
      "eval/num_tokens": 119325,
      "eval/runtime": 5.092,
      "eval/samples_per_second": 7.855,
      "eval/steps_per_second": 0.982,
      "total_flos": 6216038131580928,
      "train/entropy": 0.5208225250244141,
      "train/epoch": 3,
      "train/global_step": 60,
      "train/grad_norm": 0.23828125,
      "train/learning_rate": 1.6918417287318245e-07,
      "train/loss": 0.4839335918426514,
      "train/mean_token_accuracy": 0.876083517074585,
      "train/num_tokens": 119325,
      "train_loss": 0.7223183552424113,
      "train_runtime": 204.2421,
      "train_samples_per_second": 2.35,
      "train_steps_per_second": 0.294
    },
    "url": "https://wandb.ai/nolancacheux/hackathon-london-nolan-2026/runs/ydr264lg",
    "created_at": "2026-02-28T16:51:50Z",
    "job_type": null
  },
  {
    "name": "ecotopia-citizens-ministral-8b",
    "state": "finished",
    "config": {
      "bf16": true,
      "fp16": false,
      "fsdp": [],
      "seed": 42,
      "tf32": null,
      "debug": [],
      "dtype": "bfloat16",
      "optim": "adamw_torch_fused",
      "do_eval": true,
      "packing": false,
      "project": "huggingface",
      "use_cpu": false,
      "do_train": false,
      "head_dim": 128,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "run_name": "ecotopia-citizens-ministral-8b",
      "data_seed": null,
      "deepspeed": null,
      "eos_token": "<EOS_TOKEN>",
      "hub_token": "<HUB_TOKEN>",
      "log_level": "passive",
      "loss_type": "nll",
      "max_steps": -1,
      "pad_token": "<PAD_TOKEN>",
      "report_to": [
        "wandb"
      ],
      "use_cache": false,
      "adam_beta1": 0.9,
      "adam_beta2": 0.999,
      "do_predict": false,
      "eval_delay": 0,
      "eval_steps": 20,
      "hidden_act": "silu",
      "local_rank": -1,
      "max_length": 1024,
      "model_type": "ministral",
      "optim_args": null,
      "output_dir": "./ecotopia-citizens-ministral-8b",
      "save_steps": 50,
      "vocab_size": 131072,
      "ddp_backend": null,
      "ddp_timeout": 1800,
      "fsdp_config": {
        "xla": false,
        "xla_fsdp_v2": false,
        "min_num_params": 0,
        "xla_fsdp_grad_ckpt": false
      },
      "hidden_size": 4096,
      "label_names": null,
      "layer_types": [
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "sliding_attention",
        "sliding_attention"
      ],
      "logging_dir": null,
      "peft_config": {
        "default": {
          "r": 16,
          "bias": "none",
          "revision": null,
          "use_dora": false,
          "lora_bias": false,
          "peft_type": "LORA",
          "task_type": "CAUSAL_LM",
          "eva_config": null,
          "lora_alpha": 32,
          "use_qalora": false,
          "use_rslora": false,
          "arrow_config": null,
          "auto_mapping": null,
          "corda_config": null,
          "lora_dropout": 0.05,
          "peft_version": "0.18.1",
          "megatron_core": "megatron.core",
          "fan_in_fan_out": false,
          "inference_mode": false,
          "layers_pattern": null,
          "runtime_config": {
            "ephemeral_gpu_offload": false
          },
          "target_modules": [
            "k_proj",
            "v_proj",
            "q_proj",
            "o_proj"
          ],
          "exclude_modules": null,
          "megatron_config": null,
          "modules_to_save": null,
          "init_lora_weights": true,
          "layer_replication": null,
          "qalora_group_size": 16,
          "target_parameters": null,
          "ensure_weight_tying": false,
          "layers_to_transform": null,
          "alora_invocation_tokens": null,
          "base_model_name_or_path": "mistralai/Ministral-8B-Instruct-2410",
          "trainable_token_indices": null
        }
      },
      "push_to_hub": true,
      "return_dict": true,
      "adam_epsilon": 1e-08,
      "bos_token_id": 1,
      "disable_tqdm": false,
      "eos_token_id": 2,
      "eval_packing": null,
      "hub_model_id": "mistral-hackaton-2026/ecotopia-citizens-ministral-8b",
      "hub_revision": null,
      "hub_strategy": "end",
      "pad_token_id": 2,
      "padding_free": false,
      "problem_type": null,
      "rms_norm_eps": 1e-05,
      "warmup_ratio": 0.1,
      "warmup_steps": 0.1,
      "weight_decay": 0,
      "_name_or_path": "mistralai/Ministral-8B-Instruct-2410",
      "architectures": [
        "MistralForCausalLM"
      ],
      "eval_on_start": false,
      "eval_strategy": "steps",
      "learning_rate": 0.0002,
      "logging_steps": 5,
      "max_grad_norm": 1,
      "save_strategy": "steps",
      "torch_compile": false,
      "bf16_full_eval": false,
      "dataset_kwargs": null,
      "fp16_full_eval": false,
      "sliding_window": 32768,
      "hub_always_push": false,
      "rope_parameters": {
        "rope_type": "default",
        "rope_theta": 100000000
      },
      "save_only_model": false,
      "shuffle_dataset": false,
      "dataset_num_proc": null,
      "full_determinism": false,
      "hub_private_repo": null,
      "ignore_data_skip": false,
      "log_on_each_node": true,
      "logging_strategy": "steps",
      "num_train_epochs": 3,
      "packing_strategy": "bfd",
      "save_total_limit": null,
      "trackio_space_id": "trackio",
      "use_liger_kernel": false,
      "attention_dropout": 0,
      "ddp_bucket_cap_mb": null,
      "greater_is_better": null,
      "initializer_range": 0.02,
      "intermediate_size": 12288,
      "log_level_replica": "warning",
      "lr_scheduler_type": "cosine",
      "model_init_kwargs": null,
      "num_hidden_layers": 36,
      "output_attentions": false,
      "save_on_each_node": false,
      "accelerator_config": {
        "even_batches": true,
        "non_blocking": false,
        "split_batches": false,
        "dispatch_batches": null,
        "use_seedable_sampler": true,
        "gradient_accumulation_kwargs": null
      },
      "batch_eval_metrics": false,
      "chat_template_path": null,
      "dataset_text_field": "text",
      "is_encoder_decoder": false,
      "length_column_name": "length",
      "logging_first_step": false,
      "pad_to_multiple_of": null,
      "parallelism_config": null,
      "torch_compile_mode": null,
      "assistant_only_loss": false,
      "include_for_metrics": [],
      "liger_kernel_config": null,
      "lr_scheduler_kwargs": null,
      "neftune_noise_alpha": null,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "quantization_config": {
        "load_in_4bit": true,
        "load_in_8bit": false,
        "quant_method": "BITS_AND_BYTES",
        "_load_in_4bit": true,
        "_load_in_8bit": false,
        "llm_int8_threshold": 6,
        "bnb_4bit_quant_type": "nf4",
        "llm_int8_skip_modules": null,
        "bnb_4bit_compute_dtype": "bfloat16",
        "bnb_4bit_quant_storage": "uint8",
        "llm_int8_has_fp16_weight": false,
        "bnb_4bit_use_double_quant": true,
        "llm_int8_enable_fp32_cpu_offload": false
      },
      "skip_memory_metrics": true,
      "tie_word_embeddings": false,
      "auto_find_batch_size": false,
      "completion_only_loss": null,
      "dataloader_drop_last": false,
      "model/num_parameters": 8035143680,
      "optim_target_modules": null,
      "output_hidden_states": false,
      "prediction_loss_only": false,
      "transformers_version": "5.2.0",
      "activation_offloading": false,
      "dataloader_pin_memory": true,
      "ddp_broadcast_buffers": null,
      "enable_jit_checkpoint": false,
      "metric_for_best_model": null,
      "remove_unused_columns": true,
      "torch_compile_backend": null,
      "dataloader_num_workers": 0,
      "eval_do_concat_batches": true,
      "eval_use_gather_object": false,
      "gradient_checkpointing": true,
      "label_smoothing_factor": 0,
      "load_best_model_at_end": false,
      "logging_nan_inf_filter": true,
      "resume_from_checkpoint": null,
      "chunk_size_feed_forward": 0,
      "eval_accumulation_steps": null,
      "max_position_embeddings": 32768,
      "torch_empty_cache_steps": null,
      "train_sampling_strategy": "random",
      "dataloader_prefetch_factor": null,
      "ddp_find_unused_parameters": null,
      "per_device_eval_batch_size": 8,
      "gradient_accumulation_steps": 4,
      "per_device_train_batch_size": 2,
      "average_tokens_across_devices": true,
      "dataloader_persistent_workers": false,
      "gradient_checkpointing_kwargs": {
        "use_reentrant": false
      },
      "include_num_input_tokens_seen": "no",
      "restore_callback_states_from_checkpoint": false
    },
    "summary": {
      "_runtime": 598,
      "_step": 25,
      "_timestamp": 1772298536.8561542,
      "_wandb": "{'runtime': 598}",
      "eval/entropy": 0.2606787433226903,
      "eval/loss": 0.25792449712753296,
      "eval/mean_token_accuracy": 0.919433421558804,
      "eval/num_tokens": 350586,
      "eval/runtime": 13.8936,
      "eval/samples_per_second": 4.894,
      "eval/steps_per_second": 0.648,
      "total_flos": 18303223840505856,
      "train/entropy": 0.25496286526322365,
      "train/epoch": 3,
      "train/global_step": 102,
      "train/grad_norm": 0.1220703125,
      "train/learning_rate": 5.358475304807375e-07,
      "train/loss": 0.23261620998382568,
      "train/mean_token_accuracy": 0.9236744940280914,
      "train/num_tokens": 357216,
      "train_loss": 0.4777353692288493,
      "train_runtime": 593.374,
      "train_samples_per_second": 1.375,
      "train_steps_per_second": 0.172
    },
    "url": "https://wandb.ai/nolancacheux/hackathon-london-nolan-2026/runs/iqqdzc1m",
    "created_at": "2026-02-28T16:59:01Z",
    "job_type": null
  },
  {
    "name": "upload-ecotopia-extract-ministral-8b",
    "state": "finished",
    "config": {},
    "summary": {
      "_runtime": 4,
      "_wandb": "{'runtime': 4}"
    },
    "url": "https://wandb.ai/nolancacheux/hackathon-london-nolan-2026/runs/q0pfe51a",
    "created_at": "2026-02-28T17:20:31Z",
    "job_type": "model-upload"
  },
  {
    "name": "upload-ecotopia-citizens-ministral-8b",
    "state": "finished",
    "config": {},
    "summary": {
      "_runtime": 3,
      "_wandb": "{'runtime': 3}"
    },
    "url": "https://wandb.ai/nolancacheux/hackathon-london-nolan-2026/runs/rtbktcb6",
    "created_at": "2026-02-28T17:20:42Z",
    "job_type": "model-upload"
  }
]